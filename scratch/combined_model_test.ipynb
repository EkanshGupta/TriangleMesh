{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGCNN branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgcnn.model import get_graph_feature\n",
    "\n",
    "\n",
    "class DGCNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, opt: dict):\n",
    "        '''\n",
    "        opt structure:\n",
    "            'k': int, for k-NN\n",
    "            'emb_dims': int, used to calc hidden size for various layers\n",
    "        '''\n",
    "        super(DGCNNFeatureExtractor, self).__init__()\n",
    "        self.k = opt['k']\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(opt['emb_dims'])\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n",
    "                                   self.bn3,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n",
    "                                   self.bn4,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(512, opt['emb_dims'], kernel_size=1, bias=False),\n",
    "                                   self.bn5,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "        self.feat_dim = opt['emb_dims'] * 2\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = get_graph_feature(x, k=self.k)\n",
    "        x = self.conv1(x)\n",
    "        x1 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x1, k=self.k)\n",
    "        x = self.conv2(x)\n",
    "        x2 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x2, k=self.k)\n",
    "        x = self.conv3(x)\n",
    "        x3 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x3, k=self.k)\n",
    "        x = self.conv4(x)\n",
    "        x4 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
    "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DGCNNFeatureExtractor(\n",
       "   (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (bn5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv1): Sequential(\n",
       "     (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2)\n",
       "   )\n",
       "   (conv2): Sequential(\n",
       "     (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2)\n",
       "   )\n",
       "   (conv3): Sequential(\n",
       "     (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2)\n",
       "   )\n",
       "   (conv4): Sequential(\n",
       "     (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2)\n",
       "   )\n",
       "   (conv5): Sequential(\n",
       "     (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "     (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2)\n",
       "   )\n",
       " ),\n",
       " torch.Size([8, 256]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgcnn_opt = {\n",
    "    'k': 5,\n",
    "    'emb_dims': 128,\n",
    "}\n",
    "\n",
    "dgcnn_feat_ex = DGCNNFeatureExtractor(dgcnn_opt)\n",
    "dgcnn_feat_ex.eval()\n",
    "dgcnn_feat_ex, dgcnn_feat_ex(torch.randn((8,3,1024))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeshCNN branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meshcnn.models.networks import MResConv, get_norm_args, get_norm_layer\n",
    "from meshcnn.models.layers.mesh_pool import MeshPool\n",
    "from meshcnn.models.layers.mesh import Mesh\n",
    "\n",
    "\n",
    "class MeshCNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, opt: dict):\n",
    "        '''\n",
    "        opt structure:\n",
    "            'nf0': int\n",
    "                num input channels (5 for the usual MeshCNN initial edge features)\n",
    "                Corresponds to \"opt.input_nc\" in original code, with no default (inferred from dataset)\n",
    "\n",
    "            'conv_res': list of ints\n",
    "                num out channels (i.e. filters) for each meshconv layer\n",
    "                Corresponds to \"opt.ncf\" in original code, with default [16, 32, 32]\n",
    "\n",
    "            'input_res': int\n",
    "                num input edges (we take only this many edges from each input mesh)\n",
    "                Corresponds to \"opt.ninput_edges\" in original code, with default 750\n",
    "\n",
    "            'pool_res': list of ints\n",
    "                num edges to keep after each meshpool layer\n",
    "                Corresponds to \"opt.pool_res\" in original code, with default [1140, 780, 580] \n",
    "\n",
    "            'norm': str, one of ['batch', 'instance', 'group', 'none']\n",
    "                type of norm layer to use\n",
    "                Corresponds to \"opt.norm\" in original code, with default 'batch'\n",
    "\n",
    "            'num_groups': int\n",
    "                num of groups for groupnorm\n",
    "                Corresponds to \"opt.num_groups\" in original code, with default 16\n",
    "\n",
    "            'nresblocks': int\n",
    "                num res blocks in each mresconv\n",
    "                Corresponds to \"opt.resblocks\" in original code, with default 0\n",
    "        '''\n",
    "        super(MeshCNNFeatureExtractor, self).__init__()\n",
    "        self.k = [opt['nf0']] + opt['conv_res']\n",
    "        self.res = [opt['input_res']] + opt['pool_res']\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=opt['norm'], num_groups=opt['num_groups'])\n",
    "        norm_args = get_norm_args(norm_layer, self.k[1:])\n",
    "\n",
    "        for i, ki in enumerate(self.k[:-1]):\n",
    "            setattr(self, 'conv{}'.format(i), MResConv(ki, self.k[i + 1], opt['nresblocks']))\n",
    "            setattr(self, 'norm{}'.format(i), norm_layer(**norm_args[i]))\n",
    "            setattr(self, 'pool{}'.format(i), MeshPool(self.res[i + 1]))\n",
    "\n",
    "\n",
    "        self.gp = nn.AvgPool1d(self.res[-1])\n",
    "        # self.gp = nn.MaxPool1d(self.res[-1])\n",
    "\n",
    "        self.feat_dim = self.k[-1]\n",
    "\n",
    "    def forward(self, x, mesh):\n",
    "\n",
    "        for i in range(len(self.k) - 1):\n",
    "            x = getattr(self, 'conv{}'.format(i))(x, mesh)\n",
    "            x = F.relu(getattr(self, 'norm{}'.format(i))(x))\n",
    "            x = getattr(self, 'pool{}'.format(i))(x, mesh)\n",
    "\n",
    "        x = self.gp(x)\n",
    "        x = x.view(-1, self.k[-1])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mesh': <meshcnn.models.layers.mesh.Mesh at 0x25be3088088>,\n",
       "  'label': 0,\n",
       "  'edge_features': array([[1.57387046, 1.57369169, 1.57342915, ..., 1.57413897, 1.57166371,\n",
       "          1.57090315],\n",
       "         [1.35346367, 1.31874424, 1.4147537 , ..., 1.28607998, 1.41143283,\n",
       "          1.52148169],\n",
       "         [1.46005855, 1.49583822, 1.47602932, ..., 1.35379826, 1.48621636,\n",
       "          1.53421813],\n",
       "         [1.01332234, 0.83147991, 0.89266225, ..., 1.33598894, 0.85190091,\n",
       "          0.65474185],\n",
       "         [1.46880606, 1.35175648, 1.14274885, ..., 1.6005793 , 1.03006614,\n",
       "          0.68757185]])},\n",
       " (5, 750))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "meshcnn data loader item format - dict, with keys:\n",
    "    'mesh': Mesh class instance\n",
    "    'label': Output class label\n",
    "    'edge_features': Features extracted using extract_features() of the Mesh object above,\n",
    "                     but PADDED TO ninput_edges AND NORMALIZED BY MEAN&STD OF DATA\n",
    "'''\n",
    "\n",
    "mesh = Mesh(\n",
    "    file=r'C:\\Academic\\GT - MSCS\\Sem II - Spring 2022\\CS 7643 - DL\\Project\\src\\TriangleMesh\\data\\shrec_16\\armadillo\\test\\T55.obj',\n",
    "    opt=None, export_folder=None)\n",
    "test_data = {\n",
    "    'mesh': mesh,\n",
    "    'label': 0,\n",
    "    'edge_features': mesh.extract_features(),\n",
    "}\n",
    "test_data, test_data['edge_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 5, 750, 1), [<meshcnn.models.layers.mesh.Mesh at 0x25be3088088>])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['edge_features'][None,...,None].shape, [test_data['mesh']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MeshCNNFeatureExtractor(\n",
       "   (conv0): MResConv(\n",
       "     (conv0): MeshConv(\n",
       "       (conv): Conv2d(5, 64, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv1): MeshConv(\n",
       "       (conv): Conv2d(64, 64, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "   )\n",
       "   (norm0): GroupNorm(16, 64, eps=1e-05, affine=True)\n",
       "   (pool0): MeshPool()\n",
       "   (conv1): MResConv(\n",
       "     (conv0): MeshConv(\n",
       "       (conv): Conv2d(64, 128, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv1): MeshConv(\n",
       "       (conv): Conv2d(128, 128, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "   )\n",
       "   (norm1): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
       "   (pool1): MeshPool()\n",
       "   (conv2): MResConv(\n",
       "     (conv0): MeshConv(\n",
       "       (conv): Conv2d(128, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv1): MeshConv(\n",
       "       (conv): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "   )\n",
       "   (norm2): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "   (pool2): MeshPool()\n",
       "   (conv3): MResConv(\n",
       "     (conv0): MeshConv(\n",
       "       (conv): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv1): MeshConv(\n",
       "       (conv): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "   )\n",
       "   (norm3): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "   (pool3): MeshPool()\n",
       "   (gp): AvgPool1d(kernel_size=(180,), stride=(180,), padding=(0,))\n",
       " ),\n",
       " torch.Size([1, 256]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meshcnn_opt = {\n",
    "    'nf0': test_data['edge_features'].shape[0],\n",
    "    'conv_res': [64, 128, 256, 256],\n",
    "    'input_res': test_data['edge_features'].shape[1],\n",
    "    'pool_res': [600, 450, 300, 180],\n",
    "    'norm': 'group',\n",
    "    'num_groups': 16,\n",
    "    'nresblocks': 1,\n",
    "}\n",
    "\n",
    "meshcnn_feat_ex = MeshCNNFeatureExtractor(meshcnn_opt)\n",
    "meshcnn_feat_ex.eval()\n",
    "meshcnn_feat_ex, meshcnn_feat_ex(\n",
    "    torch.from_numpy(test_data['edge_features'][None,...,None]).float().to('cpu'),\n",
    "    [test_data['mesh']]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGCNN branch on MeshCNN style data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, dtype('float64'), (252, 3), torch.Size([1, 3, 252]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mesh.vs), mesh.vs.dtype, mesh.vs.shape, torch.from_numpy(mesh.vs.T[None,...]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgcnn_feat_ex(\n",
    "    torch.from_numpy(mesh.vs.T[None,...]).float()\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined network (pre-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, dgcnn_opt: dict, meshcnn_opt: dict):\n",
    "        super(CombinedFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.dgcnn_branch = DGCNNFeatureExtractor(dgcnn_opt)\n",
    "        self.meshcnn_branch = MeshCNNFeatureExtractor(meshcnn_opt)\n",
    "\n",
    "        self.feat_dim = self.dgcnn_branch.feat_dim + self.meshcnn_branch.feat_dim\n",
    "\n",
    "\n",
    "    def forward(self, vertex_input_batch, edge_input_batch, mesh_batch):\n",
    "        vertex_based_feats = self.dgcnn_branch(vertex_input_batch)\n",
    "        edge_based_feats = self.meshcnn_branch(edge_input_batch, mesh_batch)\n",
    "\n",
    "        out = torch.cat([vertex_based_feats, edge_based_feats], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertex_input_batch = torch.from_numpy(mesh.vs.T[None,...]).float()\n",
    "edge_input_batch = torch.from_numpy(test_data['edge_features'][None,...,None]).float()\n",
    "mesh_batch = [test_data['mesh']]\n",
    "\n",
    "combined_ex = CombinedFeatureExtractor(dgcnn_opt=dgcnn_opt, meshcnn_opt=meshcnn_opt)\n",
    "combined_ex(vertex_input_batch, edge_input_batch, mesh_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedMeshClassifier(nn.Module):\n",
    "    def __init__(self, opt: dict):\n",
    "        '''\n",
    "        opt structure:\n",
    "            'classifier_opt': dict (nested options), structure:\n",
    "                'out_block_hidden_dim': hidden layer size of output 2-layer MLP\n",
    "                'out_num_classes': num classes to classify, i.e. output layer size of output 2-layer MLP\n",
    "            'dgcnn_opt': dict (nested options), see DGCNN feat ex for details\n",
    "            'meshcnn_opt': dict (nested options), see MeshCNN feat ex for details\n",
    "        '''\n",
    "        super(CombinedMeshClassifier, self).__init__()\n",
    "\n",
    "        classifier_opt = opt['classifier_opt']\n",
    "\n",
    "        self.feat_ex = CombinedFeatureExtractor(dgcnn_opt=opt['dgcnn_opt'], meshcnn_opt=opt['meshcnn_opt'])\n",
    "        self.output_block = nn.Sequential(\n",
    "            nn.Linear(in_features=self.feat_ex.feat_dim, out_features=classifier_opt['out_block_hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=classifier_opt['out_block_hidden_dim'], out_features=classifier_opt['out_num_classes'])\n",
    "        )\n",
    "\n",
    "    def forward(self, vertex_input_batch, edge_input_batch, mesh_batch):\n",
    "        combined_feats = self.feat_ex(vertex_input_batch, edge_input_batch, mesh_batch)\n",
    "        out = self.output_block(combined_feats)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_opt = {\n",
    "    'out_block_hidden_dim': 1024,\n",
    "    'out_num_classes': 30\n",
    "}\n",
    "\n",
    "classifier = CombinedMeshClassifier(\n",
    "    opt={\n",
    "        'classifier_opt': classifier_opt,\n",
    "        'dgcnn_opt': dgcnn_opt,\n",
    "        'meshcnn_opt': meshcnn_opt,\n",
    "    }\n",
    ")\n",
    "\n",
    "classifier(vertex_input_batch, edge_input_batch, mesh_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from meshcnn.util.util import is_mesh_file, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHREC16(Dataset):\n",
    "    def __init__(self, partition, device, opt: dict):\n",
    "        '''\n",
    "        opt structure:\n",
    "            'ninput_edges': int, num edges to use for meshcnn (will pad if higher than actual)\n",
    "            'num_points': int, num verts to use for dgcnn (has to be at most the actual num verts)\n",
    "            'dataroot': str\n",
    "        '''\n",
    "        super(SHREC16, self).__init__()\n",
    "        self.partition = partition\n",
    "        self.device = device\n",
    "\n",
    "        self.ninput_edges = opt['ninput_edges']\n",
    "        self.num_points = opt['num_points']\n",
    "        self.root = opt['dataroot']\n",
    "        self.dir = os.path.join(self.root)\n",
    "        self.classes, self.class_to_idx = self.find_classes(self.dir)\n",
    "        self.paths = self.make_dataset_by_class(self.dir, self.class_to_idx, partition)\n",
    "        self.nclasses = len(self.classes)\n",
    "        self.size = len(self.paths)\n",
    "\n",
    "        self.mean = 0\n",
    "        self.std = 1\n",
    "        self.get_mean_std() # init self.mean, self.std, self.ninput_channels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index][0]\n",
    "        label = self.paths[index][1]\n",
    "        mesh = Mesh(file=path, opt=None, hold_history=False, export_folder=None)\n",
    "        pointcloud = mesh.vs[:self.num_points].T\n",
    "        meta = {'mesh': mesh, 'label': label, 'pointcloud': pointcloud}\n",
    "\n",
    "        edge_features = mesh.extract_features()\n",
    "        edge_features = pad(edge_features, self.ninput_edges)\n",
    "        meta['edge_features'] = (edge_features - self.mean) / self.std\n",
    "        return meta\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "    def get_mean_std(self):\n",
    "        \"\"\" Computes Mean and Standard Deviation from Training Data\n",
    "        If mean/std file doesn't exist, will compute one\n",
    "        :returns\n",
    "        mean: N-dimensional mean\n",
    "        std: N-dimensional standard deviation\n",
    "        ninput_channels: N\n",
    "        (here N=5)\n",
    "        \"\"\"\n",
    "\n",
    "        mean_std_cache = os.path.join(self.root, 'mean_std_cache.pkl')\n",
    "        if not os.path.isfile(mean_std_cache):\n",
    "            print('computing mean std from train data...')\n",
    "            mean, std = np.array(0), np.array(0)\n",
    "            for i, data in enumerate(self):\n",
    "                if i % 5 == 0:\n",
    "                    print('{} of {}'.format(i, self.size))\n",
    "                features = data['edge_features']\n",
    "                mean = mean + features.mean(axis=1)\n",
    "                std = std + features.std(axis=1)\n",
    "            mean = mean / (i + 1)\n",
    "            std = std / (i + 1)\n",
    "            transform_dict = {'mean': mean[:, np.newaxis], 'std': std[:, np.newaxis],\n",
    "                              'ninput_channels': len(mean)}\n",
    "            with open(mean_std_cache, 'wb') as f:\n",
    "                pickle.dump(transform_dict, f)\n",
    "            print('saved: ', mean_std_cache)\n",
    "\n",
    "        # open mean / std from file\n",
    "        with open(mean_std_cache, 'rb') as f:\n",
    "            transform_dict = pickle.load(f)\n",
    "            print('loaded mean / std from cache')\n",
    "            self.mean = transform_dict['mean']\n",
    "            self.std = transform_dict['std']\n",
    "            self.ninput_channels = transform_dict['ninput_channels']\n",
    "\n",
    "    # this is when the folders are organized by class...\n",
    "    @staticmethod\n",
    "    def find_classes(dir):\n",
    "        classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dataset_by_class(dir, class_to_idx, partition):\n",
    "        meshes = []\n",
    "        dir = os.path.expanduser(dir)\n",
    "        for target in sorted(os.listdir(dir)):\n",
    "            d = os.path.join(dir, target)\n",
    "            if not os.path.isdir(d):\n",
    "                continue\n",
    "            for root, _, fnames in sorted(os.walk(d)):\n",
    "                for fname in sorted(fnames):\n",
    "                    if is_mesh_file(fname) and (root.count(partition)==1):\n",
    "                        path = os.path.join(root, fname)\n",
    "                        item = (path, class_to_idx[target])\n",
    "                        meshes.append(item)\n",
    "        return meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def collate_fn(batch, device, is_train):\n",
    "    \"\"\"Creates mini-batch tensors\n",
    "    We should build custom collate_fn rather than using default collate_fn\n",
    "    \"\"\"\n",
    "    meta = {}\n",
    "    keys = batch[0].keys()\n",
    "    for key in keys:\n",
    "        meta.update({key: np.array([d[key] for d in batch])})\n",
    "\n",
    "    input_edge_features = torch.from_numpy(meta['edge_features']).float()\n",
    "    pointcloud = torch.from_numpy(meta['pointcloud']).float()\n",
    "    label = torch.from_numpy(meta['label']).long()\n",
    "    meta['edge_features'] = input_edge_features.to(device).requires_grad_(is_train)\n",
    "    meta['pointcloud'] = pointcloud.to(device).requires_grad_(is_train)\n",
    "    meta['label'] = label.to(device)\n",
    "    # meta['mesh'] already contains the reqd list of meshes\n",
    "    return meta\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"multi-threaded data loading\"\"\"\n",
    "\n",
    "    def __init__(self, partition, opt: dict):\n",
    "        '''\n",
    "        opt structure:\n",
    "            'gpu_ids': list of ints, or None (for cpu)\n",
    "            'batch_size': int (default: 16)\n",
    "            'max_dataset_size': int (default: inf)\n",
    "            'shuffle': bool. Whether to shuffle or not\n",
    "            'num_threads': int\n",
    "\n",
    "            'dataset_opt': dict (i.e. nested options), with structure as mentioned in class SHREC16\n",
    "        '''\n",
    "        device = torch.device('cuda:{}'.format(opt['gpu_ids'][0])) if opt['gpu_ids'] else torch.device('cpu')\n",
    "        self.dataset = SHREC16(partition, device, opt['dataset_opt'])\n",
    "\n",
    "        self.batch_size = opt['batch_size']\n",
    "        self.max_dataset_size = opt['max_dataset_size'] if opt['max_dataset_size'] else np.inf\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=opt['shuffle'],\n",
    "            num_workers=opt['num_threads'],\n",
    "            collate_fn=functools.partial(\n",
    "                collate_fn,\n",
    "                device=device, is_train=(partition=='train')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset), self.max_dataset_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, data in enumerate(self.dataloader):\n",
    "            if i * self.batch_size >= self.max_dataset_size:\n",
    "                break\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n",
      "loaded mean / std from cache\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(480, 120)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root_dir = r'C:\\Academic\\GT - MSCS\\Sem II - Spring 2022\\CS 7643 - DL\\Project\\src\\TriangleMesh\\data\\shrec_16'\n",
    "dataloader_opt = {\n",
    "    'gpu_ids': None,\n",
    "    'batch_size': 16,\n",
    "    'max_dataset_size': np.inf,\n",
    "    'shuffle': True,\n",
    "    'num_threads': 0,\n",
    "    'dataset_opt': {\n",
    "        'ninput_edges': 750,\n",
    "        'num_points': 250,\n",
    "        'dataroot': data_root_dir,\n",
    "    },\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader('train', dataloader_opt)\n",
    "test_dataloader = DataLoader('test', dataloader_opt)\n",
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 250]) torch.Size([16, 5, 750])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30])\n"
     ]
    }
   ],
   "source": [
    "for train_data, test_data in zip(train_dataloader, test_dataloader):\n",
    "    print(train_data['pointcloud'].shape, test_data['edge_features'].shape)\n",
    "\n",
    "    print(classifier(train_data['pointcloud'], train_data['edge_features'], train_data['mesh']).shape)\n",
    "    print(classifier(test_data['pointcloud'], test_data['edge_features'], test_data['mesh']).shape)\n",
    "\n",
    "    break # comment out if you want to run through whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefining model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meshcnn.util.util import print_network\n",
    "from meshcnn.models import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierWrapper:\n",
    "    def __init__(self, is_train, opt: dict):\n",
    "        '''\n",
    "        opt structure:\n",
    "            'gpu_ids': list of ints, or None (for cpu)\n",
    "            'checkpoints_dir': str, parent folder for storing models (expt name will be a subfolder here)\n",
    "            'expt_name': str, name of expt; decides where to store models etc\n",
    "            'continue_train': bool, whether to resume training from a given epoch (if training).\n",
    "            'which_epoch': str, which epoch to load if testing or resuming training (default: 'latest'; can enter a number)\n",
    "\n",
    "            'network_opt': dict (nested options) for model architecture. see CombinedMeshClassifier for details\n",
    "\n",
    "            'net_init_opt': dict (nested options) for weight init, structure:\n",
    "                'init_type': str, type of initalization to use, among [normal|xavier|kaiming|orthogonal]. 'normal' is default\n",
    "                'init_gain': float, gain used for normal/xavier/orthogonal inits. 0.02 is default\n",
    "\n",
    "            'lr_schedule_opt': dict (nexted options) for LR schedule, structure:\n",
    "                'lr': float, init LR, default 0.0002\n",
    "                'lr_policy': str, type of schedule, among lambda|step|plateau, default 'lambda'\n",
    "                'lr_decay_iters': int, decay by gamma every lr_decay_iters iterations (if step policy). default 50\n",
    "                'lr_decay_gamma': float, the gamma for decay for step policy. default 0.1\n",
    "                'epoch_count': int, the starting epoch count (if lambda policy). default 1\n",
    "                'niter': int, num epochs to be at starting LR (if lambda policy). default 100\n",
    "                'niter_decay': int, num epochs to decay LR linearly to zero (if lambda policy). default 500.\n",
    "        '''\n",
    "        self.gpu_ids = opt['gpu_ids']\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n",
    "        self.save_dir = os.path.join(opt['checkpoints_dir'], opt['expt_name'])\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.optimizer = None\n",
    "\n",
    "        # load/define networks\n",
    "        self.net = CombinedMeshClassifier(opt['network_opt'])\n",
    "        init_opt = opt['net_init_opt']\n",
    "        networks.init_net(self.net, init_opt['init_type'], init_opt['init_gain'], self.gpu_ids)\n",
    "        self.net.train(self.is_train)\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "        self.loss = None\n",
    "\n",
    "        if self.is_train:\n",
    "            lr_schedule_opt = opt['lr_schedule_opt']\n",
    "            self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr_schedule_opt['lr'], betas=(0.9, 0.999))\n",
    "            self.scheduler = networks.get_scheduler(self.optimizer, lr_schedule_opt)\n",
    "            print_network(self.net)\n",
    "\n",
    "        if not self.is_train or opt['continue_train']:\n",
    "            self.load_network(opt['which_epoch'])\n",
    "\n",
    "    def forward(self, data_batch):\n",
    "        out = self.net(data_batch['pointcloud'], data_batch['edge_features'], data_batch['mesh'])\n",
    "        return out\n",
    "\n",
    "    def backward(self, out, data_batch):\n",
    "        self.loss = self.criterion(out, data_batch['label'])\n",
    "        self.loss.backward()\n",
    "\n",
    "    def optimize_parameters(self, data_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.forward(data_batch)\n",
    "        self.backward(out, data_batch)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "##################\n",
    "\n",
    "    def load_network(self, which_epoch):\n",
    "        \"\"\"load model from disk\"\"\"\n",
    "        save_filename = '%s_net.pth' % which_epoch\n",
    "        load_path = os.path.join(self.save_dir, save_filename)\n",
    "        if not os.path.exists(load_path):\n",
    "            return  # do nothing; let the net be as it is\n",
    "\n",
    "        net = self.net\n",
    "        if isinstance(net, torch.nn.DataParallel):\n",
    "            net = net.module\n",
    "        print('loading the model from %s' % load_path)\n",
    "        state_dict = torch.load(load_path, map_location=self.device)\n",
    "        if hasattr(state_dict, '_metadata'):\n",
    "            del state_dict._metadata\n",
    "        net.load_state_dict(state_dict)\n",
    "\n",
    "    def save_network(self, which_epoch):\n",
    "        \"\"\"save model to disk\"\"\"\n",
    "        save_filename = '%s_net.pth' % (which_epoch)\n",
    "        save_path = os.path.join(self.save_dir, save_filename)\n",
    "        if self.gpu_ids and len(self.gpu_ids) > 0 and torch.cuda.is_available():\n",
    "            torch.save(self.net.module.cpu().state_dict(), save_path)\n",
    "            self.net.cuda(self.gpu_ids[0])\n",
    "        else:\n",
    "            torch.save(self.net.cpu().state_dict(), save_path)\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"update learning rate (called once every epoch)\"\"\"\n",
    "        self.scheduler.step()\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "        print('learning rate = %.7f' % lr)\n",
    "\n",
    "    def test(self, data_batch):\n",
    "        \"\"\"tests model\n",
    "        returns: number correct and total number\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(data_batch)\n",
    "            # compute number of correct\n",
    "            pred_class = out.data.max(1)[1]\n",
    "            label_class = data_batch['label']\n",
    "            correct = self.get_accuracy(pred_class, label_class)\n",
    "        return correct, len(label_class)\n",
    "\n",
    "    def get_accuracy(self, pred, labels):\n",
    "        \"\"\"computes accuracy for classification\"\"\"\n",
    "        correct = pred.eq(labels).sum()\n",
    "        return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAML based configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def read_yaml_config(yaml_config_path):\n",
    "  if yaml_config_path is None:\n",
    "    return None\n",
    "  with open(yaml_config_path, 'r') as yml_file:\n",
    "    cfg = yaml.load(yml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "  # resolve paths\n",
    "  def resolve_paths(obj):\n",
    "    if isinstance(obj, str):\n",
    "      return (\n",
    "        os.path.abspath(os.path.join(\n",
    "          os.path.dirname(yaml_config_path), obj\n",
    "        ))\n",
    "        if ('/' in obj or '\\\\' in obj)\n",
    "        else obj\n",
    "      )\n",
    "    elif isinstance(obj, dict):\n",
    "      for k,v in obj.items():\n",
    "        obj[k] = resolve_paths(v)\n",
    "      return obj\n",
    "    elif hasattr(obj, '__iter__'):\n",
    "      for i, x in enumerate(obj):\n",
    "        obj[i] = resolve_paths(x)\n",
    "      return obj\n",
    "    else:\n",
    "      return obj\n",
    "\n",
    "  cfg = resolve_paths(cfg)\n",
    "  return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataloader_opt': {'gpu_ids': None,\n",
       "  'batch_size': 16,\n",
       "  'max_dataset_size': None,\n",
       "  'shuffle': True,\n",
       "  'num_threads': 0,\n",
       "  'dataset_opt': {'ninput_edges': 750,\n",
       "   'num_points': 250,\n",
       "   'dataroot': 'C:\\\\Academic\\\\GT - MSCS\\\\Sem II - Spring 2022\\\\CS 7643 - DL\\\\Project\\\\src\\\\TriangleMesh\\\\data\\\\shrec_16'}},\n",
       " 'recording_opt': {'print_freq': 10,\n",
       "  'save_latest_freq': 250,\n",
       "  'save_epoch_freq': 1,\n",
       "  'run_test_freq': 1},\n",
       " 'model_wrapper_opt': {'gpu_ids': None,\n",
       "  'checkpoints_dir': 'C:\\\\Academic\\\\GT - MSCS\\\\Sem II - Spring 2022\\\\CS 7643 - DL\\\\Project\\\\src\\\\TriangleMesh\\\\outputs\\\\checkpoints',\n",
       "  'expt_name': 'combine_mesh_dg--pilot',\n",
       "  'continue_train': False,\n",
       "  'which_epoch': 'latest',\n",
       "  'network_opt': {'dgcnn_opt': {'k': 5, 'emb_dims': 128},\n",
       "   'meshcnn_opt': {'nf0': 5,\n",
       "    'conv_res': [64, 128, 256, 256],\n",
       "    'input_res': 750,\n",
       "    'pool_res': [600, 450, 300, 180],\n",
       "    'norm': 'group',\n",
       "    'num_groups': 16,\n",
       "    'nresblocks': 1},\n",
       "   'classifier_opt': {'out_block_hidden_dim': 1024, 'out_num_classes': 30}},\n",
       "  'net_init_opt': {'init_type': 'normal', 'init_gain': 0.02},\n",
       "  'lr_schedule_opt': {'lr': 0.0002,\n",
       "   'lr_policy': 'lambda',\n",
       "   'lr_decay_iters': 50,\n",
       "   'lr_decay_gamma': 0.1,\n",
       "   'epoch_count': 1,\n",
       "   'niter': 100,\n",
       "   'niter_decay': 500}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_yaml_path = r'C:\\Academic\\GT - MSCS\\Sem II - Spring 2022\\CS 7643 - DL\\Project\\src\\TriangleMesh\\configs\\combine_mesh_dg\\pilot.yml'\n",
    "\n",
    "opt_full = read_yaml_config(opt_yaml_path)\n",
    "opt_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test (to start with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meshcnn.util.writer import Writer\n",
    "\n",
    "def test(opt: dict, epoch=-1):\n",
    "    print('Running Test')\n",
    "\n",
    "    dataloader_opt = opt['dataloader_opt']\n",
    "    model_wrapper_opt = opt['model_wrapper_opt']\n",
    "\n",
    "    dataloader = DataLoader('test', dataloader_opt)\n",
    "    print(f\"data num classes: {dataloader.dataset.nclasses}\")\n",
    "    model = ClassifierWrapper(False, model_wrapper_opt)\n",
    "    writer = Writer(False, model_wrapper_opt['checkpoints_dir'], model_wrapper_opt['expt_name'])\n",
    "    # test\n",
    "    writer.reset_counter()\n",
    "    for i, data in enumerate(dataloader):\n",
    "        ncorrect, nexamples = model.test(data)\n",
    "        writer.update_counter(ncorrect, nexamples)\n",
    "    writer.print_acc(epoch, writer.acc)\n",
    "    return writer.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test\n",
      "loaded mean / std from cache\n",
      "data num classes: 30\n",
      "loading the model from C:\\Academic\\GT - MSCS\\Sem II - Spring 2022\\CS 7643 - DL\\Project\\src\\TriangleMesh\\outputs\\checkpoints\\combine_mesh_dg--pilot\\latest_net.pth\n",
      "epoch: -1, TEST ACC: [4.1667 %]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.041666666666666664"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(opt_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train(opt: dict):\n",
    "    dataloader_opt = opt['dataloader_opt']\n",
    "    model_wrapper_opt = opt['model_wrapper_opt']\n",
    "    recording_opt = opt['recording_opt']\n",
    "\n",
    "    dataloader = DataLoader('train', dataloader_opt)\n",
    "    dataset_size = len(dataloader)\n",
    "    print('#training meshes = %d' % dataset_size)\n",
    "\n",
    "    model = ClassifierWrapper(True, model_wrapper_opt)\n",
    "    writer = Writer(True, model_wrapper_opt['checkpoints_dir'], model_wrapper_opt['expt_name'])\n",
    "\n",
    "    lr_schedule_opt = model_wrapper_opt['lr_schedule_opt']\n",
    "    batch_size = dataloader_opt['batch_size']\n",
    "\n",
    "    total_steps = 0\n",
    "    for epoch in range(lr_schedule_opt['epoch_count'], lr_schedule_opt['niter'] + lr_schedule_opt['niter_decay'] + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        iter_data_time = time.time()\n",
    "        epoch_iter = 0\n",
    "\n",
    "        for i, data in enumerate(dataloader):\n",
    "            iter_start_time = time.time()\n",
    "            if total_steps % recording_opt['print_freq'] == 0:\n",
    "                t_data = iter_start_time - iter_data_time\n",
    "            total_steps += batch_size\n",
    "            epoch_iter += batch_size\n",
    "            model.optimize_parameters(data)\n",
    "\n",
    "            if total_steps % recording_opt['print_freq'] == 0:\n",
    "                loss = model.loss\n",
    "                t = (time.time() - iter_start_time) / batch_size\n",
    "                writer.print_current_losses(epoch, epoch_iter, loss, t, t_data)\n",
    "                writer.plot_loss(loss, epoch, epoch_iter, dataset_size)\n",
    "\n",
    "            if i % recording_opt['save_latest_freq'] == 0:\n",
    "                print('saving the latest model (epoch %d, total_steps %d)' %\n",
    "                        (epoch, total_steps))\n",
    "                model.save_network('latest')\n",
    "\n",
    "            iter_data_time = time.time()\n",
    "        if epoch % recording_opt['save_epoch_freq'] == 0:\n",
    "            print('saving the model at the end of epoch %d, iters %d' %\n",
    "                    (epoch, total_steps))\n",
    "            model.save_network('latest')\n",
    "            model.save_network(epoch)\n",
    "\n",
    "        print('End of epoch %d / %d \\t Time Taken: %d sec' %\n",
    "                (epoch, lr_schedule_opt['niter'] + lr_schedule_opt['niter_decay'], time.time() - epoch_start_time))\n",
    "        model.update_learning_rate()\n",
    "\n",
    "        if epoch % recording_opt['run_test_freq'] == 0:\n",
    "            acc = test(epoch=epoch, opt=opt)\n",
    "            writer.plot_acc(acc, epoch)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(opt_full)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99bfbf8cecc21efa752308851eecf370eca7f2fbb7f44bd66e3ba553b0332285"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dl_project__cpuonly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
