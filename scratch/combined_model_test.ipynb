{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGCNN branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgcnn.model import get_graph_feature\n",
    "\n",
    "\n",
    "class DGCNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, args: dict):\n",
    "        '''\n",
    "        args must contain:\n",
    "            'k'\n",
    "            'emb_dims'\n",
    "        '''\n",
    "        super(DGCNNFeatureExtractor, self).__init__()\n",
    "        self.args = args\n",
    "        self.k = args['k']\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(args['emb_dims'])\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n",
    "                                   self.bn3,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n",
    "                                   self.bn4,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(512, args['emb_dims'], kernel_size=1, bias=False),\n",
    "                                   self.bn5,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "        self.feat_dim = args['emb_dims'] * 2\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = get_graph_feature(x, k=self.k)\n",
    "        x = self.conv1(x)\n",
    "        x1 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x1, k=self.k)\n",
    "        x = self.conv2(x)\n",
    "        x2 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x2, k=self.k)\n",
    "        x = self.conv3(x)\n",
    "        x3 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x3, k=self.k)\n",
    "        x = self.conv4(x)\n",
    "        x4 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
    "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DGCNNFeatureExtractor(\n",
       "   (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (bn5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv1): Sequential(\n",
       "     (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2)\n",
       "   )\n",
       "   (conv2): Sequential(\n",
       "     (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2)\n",
       "   )\n",
       "   (conv3): Sequential(\n",
       "     (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2)\n",
       "   )\n",
       "   (conv4): Sequential(\n",
       "     (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2)\n",
       "   )\n",
       "   (conv5): Sequential(\n",
       "     (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "     (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2)\n",
       "   )\n",
       " ),\n",
       " torch.Size([8, 256]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgcnn_args = {\n",
    "    'k': 5,\n",
    "    'emb_dims': 128,\n",
    "}\n",
    "\n",
    "dgcnn_feat_ex = DGCNNFeatureExtractor(dgcnn_args)\n",
    "dgcnn_feat_ex.eval()\n",
    "dgcnn_feat_ex, dgcnn_feat_ex(torch.randn((8,3,1024))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeshCNN branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meshcnn.models.networks import MResConv, get_norm_args, get_norm_layer\n",
    "from meshcnn.models.layers.mesh_pool import MeshPool\n",
    "from meshcnn.models.layers.mesh import Mesh\n",
    "\n",
    "\n",
    "class MeshCNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, args: dict):\n",
    "        '''\n",
    "        args must contain:\n",
    "            'nf0': int\n",
    "                num input channels (5 for the usual MeshCNN initial edge features)\n",
    "                Corresponds to \"opt.input_nc\" in original code, with no default (inferred from dataset)\n",
    "\n",
    "            'conv_res': list of ints\n",
    "                num out channels (i.e. filters) for each meshconv layer\n",
    "                Corresponds to \"opt.ncf\" in original code, with default [16, 32, 32]\n",
    "\n",
    "            'input_res': int\n",
    "                num input edges (we take only this many edges from each input mesh)\n",
    "                Corresponds to \"opt.ninput_edges\" in original code, with default 750\n",
    "\n",
    "            'pool_res': list of ints\n",
    "                num edges to keep after each meshpool layer\n",
    "                Corresponds to \"opt.pool_res\" in original code, with default [1140, 780, 580] \n",
    "\n",
    "            'norm': str, one of ['batch', 'instance', 'group', 'none']\n",
    "                type of norm layer to use\n",
    "                Corresponds to \"opt.norm\" in original code, with default 'batch'\n",
    "\n",
    "            'num_groups': int\n",
    "                num of groups for groupnorm\n",
    "                Corresponds to \"opt.num_groups\" in original code, with default 16\n",
    "\n",
    "            'nresblocks': int\n",
    "                num res blocks in each mresconv\n",
    "                Corresponds to \"opt.resblocks\" in original code, with default 0\n",
    "        '''\n",
    "        super(MeshCNNFeatureExtractor, self).__init__()\n",
    "        self.k = [args['nf0']] + args['conv_res']\n",
    "        self.res = [args['input_res']] + args['pool_res']\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=args['norm'], num_groups=args['num_groups'])\n",
    "        norm_args = get_norm_args(norm_layer, self.k[1:])\n",
    "\n",
    "        for i, ki in enumerate(self.k[:-1]):\n",
    "            setattr(self, 'conv{}'.format(i), MResConv(ki, self.k[i + 1], args['nresblocks']))\n",
    "            setattr(self, 'norm{}'.format(i), norm_layer(**norm_args[i]))\n",
    "            setattr(self, 'pool{}'.format(i), MeshPool(self.res[i + 1]))\n",
    "\n",
    "\n",
    "        self.gp = nn.AvgPool1d(self.res[-1])\n",
    "        # self.gp = nn.MaxPool1d(self.res[-1])\n",
    "\n",
    "        self.feat_dim = self.k[-1]\n",
    "\n",
    "    def forward(self, x, mesh):\n",
    "\n",
    "        for i in range(len(self.k) - 1):\n",
    "            x = getattr(self, 'conv{}'.format(i))(x, mesh)\n",
    "            x = F.relu(getattr(self, 'norm{}'.format(i))(x))\n",
    "            x = getattr(self, 'pool{}'.format(i))(x, mesh)\n",
    "\n",
    "        x = self.gp(x)\n",
    "        x = x.view(-1, self.k[-1])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mesh': <meshcnn.models.layers.mesh.Mesh at 0x299a44358c8>,\n",
       "  'label': 0,\n",
       "  'edge_features': array([[1.57387046, 1.57369169, 1.57342915, ..., 1.57413897, 1.57166371,\n",
       "          1.57090315],\n",
       "         [1.35346367, 1.31874424, 1.4147537 , ..., 1.28607998, 1.41143283,\n",
       "          1.52148169],\n",
       "         [1.46005855, 1.49583822, 1.47602932, ..., 1.35379826, 1.48621636,\n",
       "          1.53421813],\n",
       "         [1.01332234, 0.83147991, 0.89266225, ..., 1.33598894, 0.85190091,\n",
       "          0.65474185],\n",
       "         [1.46880606, 1.35175648, 1.14274885, ..., 1.6005793 , 1.03006614,\n",
       "          0.68757185]])},\n",
       " (5, 750))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "meshcnn data loader item format - dict, with keys:\n",
    "    'mesh': Mesh class instance\n",
    "    'label': Output class label\n",
    "    'edge_features': Features extracted using extract_features() of the Mesh object above,\n",
    "                     but PADDED TO ninput_edges AND NORMALIZED BY MEAN&STD OF DATA\n",
    "'''\n",
    "\n",
    "mesh = Mesh(\n",
    "    file=r'C:\\Academic\\GT - MSCS\\Sem II - Spring 2022\\CS 7643 - DL\\Project\\src\\dgcnn\\pytorch\\data\\shrec_16\\armadillo\\test\\T55.obj',\n",
    "    opt=None, export_folder=None)\n",
    "test_data = {\n",
    "    'mesh': mesh,\n",
    "    'label': 0,\n",
    "    'edge_features': mesh.extract_features(),\n",
    "}\n",
    "test_data, test_data['edge_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 5, 750, 1), [<meshcnn.models.layers.mesh.Mesh at 0x299a44358c8>])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['edge_features'][None,...,None].shape, [test_data['mesh']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MeshCNNFeatureExtractor(\n",
       "   (conv0): MResConv(\n",
       "     (conv0): MeshConv(\n",
       "       (conv): Conv2d(5, 64, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv1): MeshConv(\n",
       "       (conv): Conv2d(64, 64, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "   )\n",
       "   (norm0): GroupNorm(16, 64, eps=1e-05, affine=True)\n",
       "   (pool0): MeshPool()\n",
       "   (conv1): MResConv(\n",
       "     (conv0): MeshConv(\n",
       "       (conv): Conv2d(64, 128, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv1): MeshConv(\n",
       "       (conv): Conv2d(128, 128, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "   )\n",
       "   (norm1): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
       "   (pool1): MeshPool()\n",
       "   (conv2): MResConv(\n",
       "     (conv0): MeshConv(\n",
       "       (conv): Conv2d(128, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv1): MeshConv(\n",
       "       (conv): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "   )\n",
       "   (norm2): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "   (pool2): MeshPool()\n",
       "   (conv3): MResConv(\n",
       "     (conv0): MeshConv(\n",
       "       (conv): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv1): MeshConv(\n",
       "       (conv): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), bias=False)\n",
       "     )\n",
       "   )\n",
       "   (norm3): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "   (pool3): MeshPool()\n",
       "   (gp): AvgPool1d(kernel_size=(180,), stride=(180,), padding=(0,))\n",
       " ),\n",
       " torch.Size([1, 256]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meshcnn_args = {\n",
    "    'nf0': test_data['edge_features'].shape[0],\n",
    "    'conv_res': [64, 128, 256, 256],\n",
    "    'input_res': test_data['edge_features'].shape[1],\n",
    "    'pool_res': [600, 450, 300, 180],\n",
    "    'norm': 'group',\n",
    "    'num_groups': 16,\n",
    "    'nresblocks': 1,\n",
    "}\n",
    "\n",
    "meshcnn_feat_ex = MeshCNNFeatureExtractor(meshcnn_args)\n",
    "meshcnn_feat_ex.eval()\n",
    "meshcnn_feat_ex, meshcnn_feat_ex(\n",
    "    torch.from_numpy(test_data['edge_features'][None,...,None]).float().to('cpu'),\n",
    "    [test_data['mesh']]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGCNN branch on MeshCNN style data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, dtype('float64'), (252, 3), torch.Size([1, 3, 252]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mesh.vs), mesh.vs.dtype, mesh.vs.shape, torch.from_numpy(mesh.vs.T[None,...]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgcnn_feat_ex(\n",
    "    torch.from_numpy(mesh.vs.T[None,...]).float()\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined network (pre-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, dgcnn_args: dict, meshcnn_args: dict):\n",
    "        super(CombinedFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.dgcnn_branch = DGCNNFeatureExtractor(dgcnn_args)\n",
    "        self.meshcnn_branch = MeshCNNFeatureExtractor(meshcnn_args)\n",
    "\n",
    "        self.feat_dim = self.dgcnn_branch.feat_dim + self.meshcnn_branch.feat_dim\n",
    "\n",
    "\n",
    "    def forward(self, vertex_input_batch, edge_input_batch, mesh_batch):\n",
    "        vertex_based_feats = self.dgcnn_branch(vertex_input_batch)\n",
    "        edge_based_feats = self.meshcnn_branch(edge_input_batch, mesh_batch)\n",
    "\n",
    "        out = torch.cat([vertex_based_feats, edge_based_feats], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertex_input_batch = torch.from_numpy(mesh.vs.T[None,...]).float()\n",
    "edge_input_batch = torch.from_numpy(test_data['edge_features'][None,...,None]).float()\n",
    "mesh_batch = [test_data['mesh']]\n",
    "\n",
    "combined_ex = CombinedFeatureExtractor(dgcnn_args=dgcnn_args, meshcnn_args=meshcnn_args)\n",
    "combined_ex(vertex_input_batch, edge_input_batch, mesh_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedMeshClassifier(nn.Module):\n",
    "    def __init__(self, classifier_args: dict, dgcnn_args: dict, meshcnn_args: dict):\n",
    "        super(CombinedMeshClassifier, self).__init__()\n",
    "\n",
    "        self.feat_ex = CombinedFeatureExtractor(dgcnn_args=dgcnn_args, meshcnn_args=meshcnn_args)\n",
    "        self.output_block = nn.Sequential(\n",
    "            nn.Linear(in_features=self.feat_ex.feat_dim, out_features=classifier_args['out_block_hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=classifier_args['out_block_hidden_dim'], out_features=classifier_args['out_num_classes'])\n",
    "        )\n",
    "\n",
    "    def forward(self, vertex_input_batch, edge_input_batch, mesh_batch):\n",
    "        combined_feats = self.feat_ex(vertex_input_batch, edge_input_batch, mesh_batch)\n",
    "        out = self.output_block(combined_feats)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_args = {\n",
    "    'out_block_hidden_dim': 1024,\n",
    "    'out_num_classes': 30\n",
    "}\n",
    "\n",
    "classifier = CombinedMeshClassifier(\n",
    "    classifier_args=classifier_args,\n",
    "    dgcnn_args=dgcnn_args,\n",
    "    meshcnn_args=meshcnn_args,\n",
    ")\n",
    "\n",
    "classifier(vertex_input_batch, edge_input_batch, mesh_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from meshcnn.util.util import is_mesh_file, pad\n",
    "\n",
    "data_root_dir = r'C:\\Academic\\GT - MSCS\\Sem II - Spring 2022\\CS 7643 - DL\\Project\\src\\dgcnn\\pytorch\\data\\shrec_16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHREC16(Dataset):\n",
    "    def __init__(self, partition, device, opt: dict):\n",
    "        '''\n",
    "        opt structure:\n",
    "            'ninput_edges': int, num edges to use for meshcnn (will pad if higher than actual)\n",
    "            'num_points': int, num verts to use for dgcnn (has to be at most the actual num verts)\n",
    "            'dataroot': str\n",
    "        '''\n",
    "        super(SHREC16, self).__init__()\n",
    "        self.partition = partition\n",
    "        self.device = device\n",
    "\n",
    "        self.ninput_edges = opt['ninput_edges']\n",
    "        self.num_points = opt['num_points']\n",
    "        self.root = opt['dataroot']\n",
    "        self.dir = os.path.join(self.root)\n",
    "        self.classes, self.class_to_idx = self.find_classes(self.dir)\n",
    "        self.paths = self.make_dataset_by_class(self.dir, self.class_to_idx, partition)\n",
    "        self.nclasses = len(self.classes)\n",
    "        self.size = len(self.paths)\n",
    "\n",
    "        self.mean = 0\n",
    "        self.std = 1\n",
    "        self.get_mean_std() # init self.mean, self.std, self.ninput_channels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index][0]\n",
    "        label = self.paths[index][1]\n",
    "        mesh = Mesh(file=path, opt=None, hold_history=False, export_folder=None)\n",
    "        pointcloud = mesh.vs[:self.num_points].T\n",
    "        meta = {'mesh': mesh, 'label': label, 'pointcloud': pointcloud}\n",
    "\n",
    "        edge_features = mesh.extract_features()\n",
    "        edge_features = pad(edge_features, self.ninput_edges)\n",
    "        meta['edge_features'] = (edge_features - self.mean) / self.std\n",
    "        return meta\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "    def get_mean_std(self):\n",
    "        \"\"\" Computes Mean and Standard Deviation from Training Data\n",
    "        If mean/std file doesn't exist, will compute one\n",
    "        :returns\n",
    "        mean: N-dimensional mean\n",
    "        std: N-dimensional standard deviation\n",
    "        ninput_channels: N\n",
    "        (here N=5)\n",
    "        \"\"\"\n",
    "\n",
    "        mean_std_cache = os.path.join(self.root, 'mean_std_cache.pkl')\n",
    "        if not os.path.isfile(mean_std_cache):\n",
    "            print('computing mean std from train data...')\n",
    "            mean, std = np.array(0), np.array(0)\n",
    "            for i, data in enumerate(self):\n",
    "                if i % 5 == 0:\n",
    "                    print('{} of {}'.format(i, self.size))\n",
    "                features = data['edge_features']\n",
    "                mean = mean + features.mean(axis=1)\n",
    "                std = std + features.std(axis=1)\n",
    "            mean = mean / (i + 1)\n",
    "            std = std / (i + 1)\n",
    "            transform_dict = {'mean': mean[:, np.newaxis], 'std': std[:, np.newaxis],\n",
    "                              'ninput_channels': len(mean)}\n",
    "            with open(mean_std_cache, 'wb') as f:\n",
    "                pickle.dump(transform_dict, f)\n",
    "            print('saved: ', mean_std_cache)\n",
    "\n",
    "        # open mean / std from file\n",
    "        with open(mean_std_cache, 'rb') as f:\n",
    "            transform_dict = pickle.load(f)\n",
    "            print('loaded mean / std from cache')\n",
    "            self.mean = transform_dict['mean']\n",
    "            self.std = transform_dict['std']\n",
    "            self.ninput_channels = transform_dict['ninput_channels']\n",
    "\n",
    "    # this is when the folders are organized by class...\n",
    "    @staticmethod\n",
    "    def find_classes(dir):\n",
    "        classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dataset_by_class(dir, class_to_idx, partition):\n",
    "        meshes = []\n",
    "        dir = os.path.expanduser(dir)\n",
    "        for target in sorted(os.listdir(dir)):\n",
    "            d = os.path.join(dir, target)\n",
    "            if not os.path.isdir(d):\n",
    "                continue\n",
    "            for root, _, fnames in sorted(os.walk(d)):\n",
    "                for fname in sorted(fnames):\n",
    "                    if is_mesh_file(fname) and (root.count(partition)==1):\n",
    "                        path = os.path.join(root, fname)\n",
    "                        item = (path, class_to_idx[target])\n",
    "                        meshes.append(item)\n",
    "        return meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def collate_fn(batch, device, is_train):\n",
    "    \"\"\"Creates mini-batch tensors\n",
    "    We should build custom collate_fn rather than using default collate_fn\n",
    "    \"\"\"\n",
    "    meta = {}\n",
    "    keys = batch[0].keys()\n",
    "    for key in keys:\n",
    "        meta.update({key: np.array([d[key] for d in batch])})\n",
    "\n",
    "    input_edge_features = torch.from_numpy(meta['edge_features']).float()\n",
    "    pointcloud = torch.from_numpy(meta['pointcloud']).float()\n",
    "    label = torch.from_numpy(meta['label']).long()\n",
    "    meta['edge_features'] = input_edge_features.to(device).requires_grad_(is_train)\n",
    "    meta['pointcloud'] = pointcloud.to(device).requires_grad_(is_train)\n",
    "    meta['label'] = label.to(device)\n",
    "    # meta['mesh'] already contains the reqd list of meshes\n",
    "    return meta\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"multi-threaded data loading\"\"\"\n",
    "\n",
    "    def __init__(self, partition, opt: dict):\n",
    "        '''\n",
    "        opt structure:\n",
    "            'gpu_ids': list of ints, or None (for cpu)\n",
    "            'batch_size': int (default: 16)\n",
    "            'max_dataset_size': int (default: inf)\n",
    "            'shuffle': bool. Whether to shuffle or not\n",
    "            'num_threads': int\n",
    "\n",
    "            'dataset_opt': dict (i.e. nested options), with structure as mentioned in class SHREC16\n",
    "        '''\n",
    "        device = torch.device('cuda:{}'.format(opt['gpu_ids'][0])) if opt['gpu_ids'] else torch.device('cpu')\n",
    "        self.dataset = SHREC16(partition, device, opt['dataset_opt'])\n",
    "\n",
    "        self.batch_size = opt['batch_size']\n",
    "        self.max_dataset_size = opt['max_dataset_size']\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=opt['shuffle'],\n",
    "            num_workers=opt['num_threads'],\n",
    "            collate_fn=functools.partial(\n",
    "                collate_fn,\n",
    "                device=device, is_train=(partition=='train')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset), self.max_dataset_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, data in enumerate(self.dataloader):\n",
    "            if i * self.batch_size >= self.max_dataset_size:\n",
    "                break\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n",
      "loaded mean / std from cache\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(480, 120)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_args = {\n",
    "    'gpu_ids': None,\n",
    "    'batch_size': 16,\n",
    "    'max_dataset_size': np.inf,\n",
    "    'shuffle': True,\n",
    "    'num_threads': 0,\n",
    "    'dataset_opt': {\n",
    "        'ninput_edges': 750,\n",
    "        'num_points': 250,\n",
    "        'dataroot': data_root_dir,\n",
    "    },\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader('train', dataloader_args)\n",
    "test_dataloader = DataLoader('test', dataloader_args)\n",
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 250]) torch.Size([16, 5, 750])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 3, 250]) torch.Size([16, 5, 750])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 3, 250]) torch.Size([16, 5, 750])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 3, 250]) torch.Size([16, 5, 750])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 3, 250]) torch.Size([16, 5, 750])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 3, 250]) torch.Size([16, 5, 750])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 3, 250]) torch.Size([16, 5, 750])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 3, 250]) torch.Size([8, 5, 750])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([8, 30])\n"
     ]
    }
   ],
   "source": [
    "for train_data, test_data in zip(train_dataloader, test_dataloader):\n",
    "    print(train_data['pointcloud'].shape, test_data['edge_features'].shape)\n",
    "\n",
    "    print(classifier(train_data['pointcloud'], train_data['edge_features'], train_data['mesh']).shape)\n",
    "    print(classifier(test_data['pointcloud'], test_data['edge_features'], test_data['mesh']).shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99bfbf8cecc21efa752308851eecf370eca7f2fbb7f44bd66e3ba553b0332285"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dl_project__cpuonly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
